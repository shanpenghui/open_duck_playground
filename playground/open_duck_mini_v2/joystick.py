# Copyright 2025 DeepMind Technologies Limited
# Copyright 2025 Antoine Pirrone - Steve Nguyen
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Joystick task for Open Duck Mini V2. (based on Berkeley Humanoid)"""

from typing import Any, Dict, Optional, Union
import jax
import jax.numpy as jp
from ml_collections import config_dict
from mujoco import mjx
from mujoco.mjx._src import math
import numpy as np

from mujoco_playground._src import mjx_env
from mujoco_playground._src.collision import geoms_colliding

from . import constants
from . import base as open_duck_mini_v2_base

# from playground.common.utils import LowPassActionFilter
from playground.common.poly_reference_motion import PolyReferenceMotion
from playground.common.rewards import (
    reward_tracking_lin_vel,
    reward_tracking_ang_vel,
    cost_torques,
    cost_action_rate,
    cost_stand_still,
    reward_alive,
)
from playground.open_duck_mini_v2.custom_rewards import reward_imitation

# if set to false, won't require the reference data to be present and won't compute the reference motions polynoms for nothing
USE_IMITATION_REWARD = True
USE_MOTOR_SPEED_LIMITS = True


def default_config() -> config_dict.ConfigDict:
    return config_dict.create(
        ctrl_dt=0.02,
        sim_dt=0.002,
        episode_length=1000,
        action_repeat=1,
        action_scale=0.25,
        dof_vel_scale=0.05,
        history_len=0,
        soft_joint_pos_limit_factor=0.95,
        max_motor_velocity=5.24,  # rad/s
        noise_config=config_dict.create(
            level=1.0,  # Set to 0.0 to disable noise.
            action_min_delay=0,  # env steps
            action_max_delay=3,  # env steps
            imu_min_delay=0,  # env steps
            imu_max_delay=3,  # env steps
            scales=config_dict.create(
                hip_pos=0.03,  # rad, for each hip joint
                knee_pos=0.05,  # rad, for each knee joint
                ankle_pos=0.08,  # rad, for each ankle joint
                joint_vel=2.5,  # rad/s # Was 1.5
                gravity=0.1,
                linvel=0.1,
                gyro=0.1,
                accelerometer=0.05,
            ),
        ),
        reward_config=config_dict.create(
            scales=config_dict.create(
                tracking_lin_vel=2.5,
                tracking_ang_vel=6.0,
                torques=-1.0e-3,
                action_rate=-0.5,  # was -1.5
                stand_still=-0.2,  # was -1.0Â TODO try to relax this a bit ?
                alive=20.0,
                imitation=1.0,
            ),
            tracking_sigma=0.01,  # was working at 0.01
        ),
        push_config=config_dict.create(
            enable=True,
            interval_range=[5.0, 10.0],
            magnitude_range=[0.1, 1.0],
        ),
        lin_vel_x=[-0.15, 0.15],
        lin_vel_y=[-0.2, 0.2],
        ang_vel_yaw=[-1.0, 1.0],  # [-1.0, 1.0]
        neck_pitch_range=[-0.34, 1.1],
        head_pitch_range=[-0.78, 0.78],
        head_yaw_range=[-1.5, 1.5],
        head_roll_range=[-0.5, 0.5],
        head_range_factor=1.0,  # to make it easier
    )


class Joystick(open_duck_mini_v2_base.OpenDuckMiniV2Env):
    """Track a joystick command."""

    def __init__(
        self,
        task: str = "flat_terrain",
        config: config_dict.ConfigDict = default_config(),
        config_overrides: Optional[Dict[str, Union[str, int, list[Any]]]] = None,
    ):
        super().__init__(
            xml_path=constants.task_to_xml(task).as_posix(),
            config=config,
            config_overrides=config_overrides,
        )
        self._post_init()

    def _post_init(self) -> None:

        self._init_q = jp.array(self._mj_model.keyframe("home").qpos)
        self._default_actuator = self._mj_model.keyframe(
            "home"
        ).ctrl  # ctrl of all the actual joints (no floating base and no backlash)

        if USE_IMITATION_REWARD:
            self.PRM = PolyReferenceMotion(
                "playground/open_duck_mini_v2/data/polynomial_coefficients.pkl"
            )

        # Note: First joint is freejoint.
        # get the range of the joints
        self._lowers, self._uppers = self.mj_model.jnt_range[1:].T
        c = (self._lowers + self._uppers) / 2
        r = self._uppers - self._lowers
        self._soft_lowers = c - 0.5 * r * self._config.soft_joint_pos_limit_factor
        self._soft_uppers = c + 0.5 * r * self._config.soft_joint_pos_limit_factor

        # weights for computing the cost of each joints compared to a reference pose
        self._weights = jp.array(
            [
                1.0,
                1.0,
                0.01,
                0.01,
                1.0,  # left leg.
                # 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, #head
                1.0,
                1.0,
                0.01,
                0.01,
                1.0,  # right leg.
            ]
        )

        self._njoints = self._mj_model.njnt  # number of joints
        self._actuators = self._mj_model.nu  # number of actuators

        self._torso_body_id = self._mj_model.body(constants.ROOT_BODY).id
        self._torso_mass = self._mj_model.body_subtreemass[self._torso_body_id]
        self._site_id = self._mj_model.site("imu").id

        self._feet_site_id = np.array(
            [self._mj_model.site(name).id for name in constants.FEET_SITES]
        )
        self._floor_geom_id = self._mj_model.geom("floor").id
        self._feet_geom_id = np.array(
            [self._mj_model.geom(name).id for name in constants.FEET_GEOMS]
        )

        foot_linvel_sensor_adr = []
        for site in constants.FEET_SITES:
            sensor_id = self._mj_model.sensor(f"{site}_global_linvel").id
            sensor_adr = self._mj_model.sensor_adr[sensor_id]
            sensor_dim = self._mj_model.sensor_dim[sensor_id]
            foot_linvel_sensor_adr.append(
                list(range(sensor_adr, sensor_adr + sensor_dim))
            )
        self._foot_linvel_sensor_adr = jp.array(foot_linvel_sensor_adr)

        # # noise in the simu?
        qpos_noise_scale = np.zeros(self._actuators)

        hip_ids = [
            idx for idx, j in enumerate(constants.JOINTS_ORDER_NO_HEAD) if "_hip" in j
        ]
        knee_ids = [
            idx for idx, j in enumerate(constants.JOINTS_ORDER_NO_HEAD) if "_knee" in j
        ]
        ankle_ids = [
            idx for idx, j in enumerate(constants.JOINTS_ORDER_NO_HEAD) if "_ankle" in j
        ]

        qpos_noise_scale[hip_ids] = self._config.noise_config.scales.hip_pos
        qpos_noise_scale[knee_ids] = self._config.noise_config.scales.knee_pos
        qpos_noise_scale[ankle_ids] = self._config.noise_config.scales.ankle_pos
        # qpos_noise_scale[faa_ids] = self._config.noise_config.scales.faa_pos
        self._qpos_noise_scale = jp.array(qpos_noise_scale)

        # self.action_filter = LowPassActionFilter(
        #     1 / self._config.ctrl_dt, cutoff_frequency=37.5
        # )

    # åŠŸèƒ½ç®€è¿°ï¼š
    #
    # é‡ç½®çŽ¯å¢ƒï¼ŒåŒ…æ‹¬è®¾ç½®åˆå§‹ä½å§¿ã€é€Ÿåº¦ã€æŽ§åˆ¶å‘½ä»¤ï¼Œä»¥åŠæ¨¡ä»¿å‚è€ƒæ•°æ®å’Œæ‰°åŠ¨åˆå§‹åŒ–ã€‚å¯¹åº”å¼ºåŒ–å­¦ä¹ ä¸­çš„ episode èµ·å§‹çŠ¶æ€æž„é€ ã€‚
    # reset() ä¸»è¦é€»è¾‘ï¼š
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚    éšæœºåˆå§‹åŒ– base pose/vel   â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚    é‡‡æ · joystick æŽ§åˆ¶å‘½ä»¤      â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚    åˆå§‹åŒ– imitation å‚è€ƒè½¨è¿¹   â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚    æž„é€  info çŠ¶æ€ / å¥–åŠ±ç»Ÿè®¡   â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚    è¿”å›ž state ç”¨äºŽ RL         â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    def reset(self, rng: jax.Array) -> mjx_env.State:
        qpos = self._init_q  # ä»Žå…³é”®å¸§â€œhomeâ€ä¸­èŽ·å–åˆå§‹çš„å®Œæ•´ä½ç½®ï¼ˆæµ®åŠ¨åŸº + å…³èŠ‚ï¼‰
        # print(f'DEBUG0 init qpos: {qpos}')
        qvel = jp.zeros(self.mjx_model.nv) # åˆå§‹åŒ–æ‰€æœ‰é€Ÿåº¦ä¸º0ï¼ˆnvä¸ºè‡ªç”±åº¦æ•°é‡ï¼‰

        # init position/orientation in environment
        # x=+U(-0.05, 0.05), y=+U(-0.05, 0.05), yaw=U(-3.14, 3.14).
        # åœ¨x/yå¹³é¢æ·»åŠ å°çš„éšæœºåç§» [-0.05, 0.05]ï¼Œæ¨¡æ‹Ÿèµ·å§‹ä½ç½®ä¸ç¡®å®šæ€§
        # ç±»ä¼¼ sim2real çš„ domain randomizationï¼ˆè¯¦è§ï¼šTobin et al., 2017. "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"ï¼‰
        rng, key = jax.random.split(rng)
        dxy = jax.random.uniform(key, (2,), minval=-0.05, maxval=0.05)

        # æå–æµ®åŠ¨åŸºçš„qposï¼Œå¹¶åŠ ä¸Šx/yçš„éšæœºåç§»
        base_qpos = self.get_floating_base_qpos(qpos)
        base_qpos = base_qpos.at[0:2].set(
            qpos[self._floating_base_qpos_addr : self._floating_base_qpos_addr + 2]
            + dxy
        )  # x y noise

        # æ·»åŠ zè½´ä¸Šçš„yawæ—‹è½¬æ‰°åŠ¨ [-Ï€, Ï€]ï¼Œå¹¶å åŠ åˆ°åŽŸå§‹å§¿æ€ä¸Šï¼ˆå››å…ƒæ•°ä¹˜æ³•ï¼‰
        # å››å…ƒæ•°ä¹˜æ³• q' = q âŠ— Î”q å®žçŽ°æ—‹è½¬æ‰°åŠ¨å åŠ ã€‚è¯¦è§ã€ŠQuaternions and Rotation Sequencesã€‹ by J. B. Kuipersã€‚
        rng, key = jax.random.split(rng)
        yaw = jax.random.uniform(key, (1,), minval=-3.14, maxval=3.14)
        quat = math.axis_angle_to_quat(jp.array([0, 0, 1]), yaw)
        new_quat = math.quat_mul(
            qpos[self._floating_base_qpos_addr + 3 : self._floating_base_qpos_addr + 7],
            quat,
        )  # yaw noise

        base_qpos = base_qpos.at[3:7].set(new_quat)

        # æ›´æ–°å®Œæ•´ qpos ä¸­çš„æµ®åŠ¨åŸºä½ç½®
        qpos = self.set_floating_base_qpos(base_qpos, qpos)
        # print(f'DEBUG1 base qpos: {qpos}')
        # init joint position
        # qpos[7:]=*U(0.0, 0.1)
        rng, key = jax.random.split(rng)

        # ç»™æ¯ä¸ªå®žé™…å…³èŠ‚æ·»åŠ éšæœºç¼©æ”¾å™ªå£° [0.5, 1.5]ï¼Œå¢žå¼ºåŠ¨ä½œå¤šæ ·æ€§
        #ðŸ“˜ã€åº”ç”¨ç›®çš„ã€‘ï¼š
        #   å¢žåŠ  exploration åˆæœŸç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚
        # multiply actual joints with noise (excluding floating base and backlash)
        qpos_j = self.get_actuator_joints_qpos(qpos) * jax.random.uniform(
            key, (self._actuators,), minval=0.5, maxval=1.5
        )
        qpos = self.set_actuator_joints_qpos(qpos_j, qpos)
        # print(f'DEBUG2 joint qpos: {qpos}')
        # init joint vel
        # d(xyzrpy)=U(-0.05, 0.05)
        # ç»™æµ®åŠ¨åŸºåŠ é€Ÿåº¦æ‰°åŠ¨ [-0.05, 0.05]ï¼Œæ¨¡æ‹Ÿåˆå§‹åŠ¨æ€å˜åŒ–
        rng, key = jax.random.split(rng)
        # qvel = qvel.at[self._floating_base_qvel_addr : self._floating_base_qvel_addr + 6].set(
        #     jax.random.uniform(key, (6,), minval=-0.5, maxval=0.5)
        # )

        qvel = self.set_floating_base_qvel(
            jax.random.uniform(key, (6,), minval=-0.05, maxval=0.05), qvel
        )
        # print(f'DEBUG3 base qvel: {qvel}')
        # åˆå§‹çš„æŽ§åˆ¶é‡è®¾ç½®ä¸ºç›®æ ‡è§’åº¦å€¼ï¼ˆå³å…³é”®å¸§ä¸­çš„actuatoré»˜è®¤ä½å§¿ï¼‰
        ctrl = self.get_actuator_joints_qpos(qpos)
        # print(f'DEBUG4 ctrl: {ctrl}')
        data = mjx_env.init(self.mjx_model, qpos=qpos, qvel=qvel, ctrl=ctrl)

        # é‡‡æ ·ä¸€ä¸ª joystick å‘½ä»¤ï¼š[vx, vy, yaw_rate, neck_pitch, head_pitch, head_yaw, head_roll]
        # ðŸŽ® æŽ§åˆ¶å‘½ä»¤ä»¿çœŸç›®æ ‡æ˜¯ï¼šè®©æœºå™¨äººå­¦ä¼šé€šè¿‡æ¨¡ä»¿è½¨è¿¹æ¥æ‰§è¡Œ joystick æç¤ºçš„æ–¹å‘æ€§è¡Œä¸ºã€‚
        rng, cmd_rng = jax.random.split(rng)
        cmd = self.sample_command(cmd_rng)

        # Sample push interval.
        # é‡‡æ ·éšæœºæŽ¨åŠ›é—´éš”ï¼ˆç”¨äºŽå¤–éƒ¨æ‰°åŠ¨æ¨¡æ‹Ÿï¼‰ï¼Œå•ä½ä¸ºä»¿çœŸç§’æ•°
        # ðŸ“˜ã€çŽ°å®žæ¨¡æ‹Ÿã€‘ï¼š
        #    ä»¿çœŸçœŸå®žæœºå™¨äººè®­ç»ƒä¸­çªç„¶é­å—æ’žå‡»ï¼ˆperturbationï¼‰çš„é²æ£’æ€§è€ƒå¯Ÿã€‚
        rng, push_rng = jax.random.split(rng)
        push_interval = jax.random.uniform(
            push_rng,
            minval=self._config.push_config.interval_range[0],
            maxval=self._config.push_config.interval_range[1],
        )
        push_interval_steps = jp.round(push_interval / self.dt).astype(jp.int32)

        # å¦‚æžœå¯ç”¨ imitation rewardï¼Œåˆ™ä»Žå¤šé¡¹å¼ä¸­èŽ·å–å½“å‰å‚è€ƒè½¨è¿¹ï¼ˆåŸºäºŽå‘½ä»¤ + ç›¸ä½ = 0ï¼‰
        #ðŸ“˜ã€è®ºæ–‡å‚è€ƒã€‘ï¼š
        #    Peng et al. 2021. "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control"
        if USE_IMITATION_REWARD:
            current_reference_motion = self.PRM.get_reference_motion(
                cmd[0], cmd[1], cmd[2], 0
            )
        else:
            current_reference_motion = jp.zeros(0)

        # info æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ agent å¯è§/éšè—çŠ¶æ€ï¼ŒåŒ…æ‹¬æ¨¡ä»¿é˜¶æ®µã€åŽ†å²åŠ¨ä½œã€IMUç¼“å­˜ç­‰
        info = {
            "rng": rng,
            "step": 0,
            "command": cmd,
            "last_act": jp.zeros(self.mjx_model.nu),
            "last_last_act": jp.zeros(self.mjx_model.nu),
            "last_last_last_act": jp.zeros(self.mjx_model.nu),
            "motor_targets": self._default_actuator,
            "feet_air_time": jp.zeros(2),
            "last_contact": jp.zeros(2, dtype=bool),
            "swing_peak": jp.zeros(2),
            # Push related.
            "push": jp.array([0.0, 0.0]),
            "push_step": 0,
            "push_interval_steps": push_interval_steps,
            # History related.
            "action_history": jp.zeros(
                self._config.noise_config.action_max_delay * self._actuators
            ),
            "imu_history": jp.zeros(self._config.noise_config.imu_max_delay * 3),
            # imitation related
            "imitation_i": 0,
            "current_reference_motion": current_reference_motion,
            "imitation_phase": jp.zeros(2),
        }

        # å¥–åŠ±é¡¹åˆå§‹åŒ–ï¼ˆæ ¹æ® reward_config ä¸­çš„ scale é¡¹è®¾å®šæŒ‡æ ‡ç§ç±»ï¼‰
        metrics = {}
        for k, v in self._config.reward_config.scales.items():
            if v != 0:
                if v > 0:
                    metrics[f"reward/{k}"] = jp.zeros(())
                else:
                    metrics[f"cost/{k}"] = jp.zeros(())
        metrics["swing_peak"] = jp.zeros(())

        # æ£€æµ‹å½“å‰æŽ¥è§¦çŠ¶æ€ï¼Œè¿”å›ž [left_contact, right_contact]
        contact = jp.array(
            [
                # ðŸ“˜ geoms_colliding() æ˜¯å°è£…çš„ collision check å‡½æ•°ï¼Œæ£€æµ‹å‡ ä½•ä½“æ˜¯å¦é‡åˆã€‚
                geoms_colliding(data, geom_id, self._floor_geom_id)
                for geom_id in self._feet_geom_id
            ]
        )
        # èŽ·å–è§‚æµ‹å‘é‡ï¼ˆstate å’Œ privileged_stateï¼‰
        obs = self._get_obs(data, info, contact)
        reward, done = jp.zeros(2)  # åˆå§‹æ— å¥–åŠ±ï¼Œæœªç»ˆæ­¢
        # æ‰“åŒ…ä¸º mjx_env.State è¿”å›žï¼Œç”¨äºŽå¼ºåŒ–å­¦ä¹ è¿­ä»£
        return mjx_env.State(data, obs, reward, done, metrics, info)

    # ä¸»çŽ¯å¢ƒæ­¥è¿›å‡½æ•°ã€‚è¾“å…¥å½“å‰çŠ¶æ€å’Œ agent åŠ¨ä½œï¼Œè¿”å›žæ–°çŠ¶æ€ã€‚
    def step(self, state: mjx_env.State, action: jax.Array) -> mjx_env.State:

        if USE_IMITATION_REWARD:
            # imitation_i è¡¨ç¤ºå½“å‰æ¨¡ä»¿å‘¨æœŸå†…çš„æ­¥æ•°ç´¢å¼•ï¼Œæ¯æ­¥é€’å¢ž
            state.info["imitation_i"] += 1
            # å¯¹æ­¥æ•°å–æ¨¡ï¼Œä¿è¯ phase å‘¨æœŸæ€§ï¼ˆä¸€ä¸ªå‘¨æœŸå†… imitation_i åœ¨ [0, nb_steps_in_period)ï¼‰
            state.info["imitation_i"] = (
                    state.info["imitation_i"] % self.PRM.nb_steps_in_period
            )  # å·²åœ¨ get_reference_motion å†…éƒ¨åšäº†æ¨¡è¿ç®—ï¼Œè¿™é‡Œæ˜¯æ˜¾å¼åŒæ­¥

            # ä½¿ç”¨ cos/sin ç¼–ç å°†æ¨¡ä»¿å‘¨æœŸä½ç½®ä¿¡æ¯è½¬åŒ–ä¸ºå¯å­¦ä¹ çš„å‘é‡ï¼ˆç›¸å½“äºŽæ—¶é—´ embeddingï¼‰
            # å¯¹åº” AMP è®ºæ–‡ä¸­ imitation phase è¡¨ç¤ºæ³•ï¼š
            # \phi_i = [cos(2Ï€ * t_i / T), sin(2Ï€ * t_i / T)]
            state.info["imitation_phase"] = jp.array(
                [
                    jp.cos(
                        (state.info["imitation_i"] / self.PRM.nb_steps_in_period)
                        * 2
                        * jp.pi
                    ),
                    jp.sin(
                        (state.info["imitation_i"] / self.PRM.nb_steps_in_period)
                        * 2
                        * jp.pi
                    ),
                ]
            )
        else:
            # imitation reward æœªå¯ç”¨æ—¶ï¼Œphase å’Œæ­¥æ•°é‡ç½®
            state.info["imitation_i"] = 0

        if USE_IMITATION_REWARD:
            # æ ¹æ® joystick æŒ‡ä»¤ vx, vy, yaw å’Œ imitation_i èŽ·å–å½“å‰å‚è€ƒåŠ¨ä½œ
            state.info["current_reference_motion"] = self.PRM.get_reference_motion(
                state.info["command"][0],  # vx
                state.info["command"][1],  # vy
                state.info["command"][2],  # yaw_rate
                state.info["imitation_i"], # å½“å‰æ¨¡ä»¿å¸§ç¼–å·
            )
        else:
            # imitation reward å…³é—­æ—¶è¿”å›žé›¶å‚è€ƒè½¨è¿¹
            state.info["current_reference_motion"] = jp.zeros(0)

        # æ‹†åˆ†éšæœºç§å­ï¼Œåˆ†åˆ«ç”¨äºŽæŽ¨åŠ›æ–¹å‘ã€æŽ¨åŠ›å¼ºåº¦ã€åŠ¨ä½œå»¶è¿Ÿ
        state.info["rng"], push1_rng, push2_rng, action_delay_rng = jax.random.split(
            state.info["rng"], 4
        )

        # æ»šåŠ¨åŠ¨ä½œåŽ†å²ç¼“å­˜ï¼ˆFIFOï¼‰ï¼Œæ’å…¥å½“å‰åŠ¨ä½œ
        action_history = (
            jp.roll(state.info["action_history"], self._actuators)
            .at[: self._actuators]
            .set(action)
        )
        state.info["action_history"] = action_history

        # ä»ŽæŒ‡å®šåŠ¨ä½œå»¶è¿ŸèŒƒå›´å†…é‡‡æ ·ä¸€ä¸ªç´¢å¼•å€¼
        action_idx = jax.random.randint(
            action_delay_rng,
            (1,),
            minval=self._config.noise_config.action_min_delay,
            maxval=self._config.noise_config.action_max_delay,
        )
        # æ ¹æ®å»¶è¿Ÿæ­¥æ•°ä»ŽåŽ†å²é˜Ÿåˆ—ä¸­å–å‡ºå®žé™…ä½¿ç”¨çš„åŠ¨ä½œï¼ˆç”¨äºŽæ¨¡æ‹ŸæŽ§åˆ¶é“¾æ¡æ—¶å»¶ï¼‰
        action_w_delay = action_history.reshape((-1, self._actuators))[action_idx[0]]

        # é‡‡æ ·æŽ¨åŠ›æ–¹å‘ï¼ˆÎ¸ âˆˆ [0, 2Ï€]ï¼‰å’ŒæŽ¨åŠ›å¤§å°ï¼ˆâˆˆ magnitude_rangeï¼‰
        push_theta = jax.random.uniform(push1_rng, maxval=2 * jp.pi)
        push_magnitude = jax.random.uniform(
            push2_rng,
            minval=self._config.push_config.magnitude_range[0],
            maxval=self._config.push_config.magnitude_range[1],
        )
        # æž„é€ å•ä½å‘é‡æ–¹å‘çš„æŽ¨åŠ›
        push = jp.array([jp.cos(push_theta), jp.sin(push_theta)])
        # ä»…åœ¨æŽ¨åŠ›æ­¥æ•°è§¦å‘ç‚¹æ–½åŠ æŽ¨åŠ›ï¼Œå¦åˆ™ä¸ºé›¶
        push *= (
                jp.mod(state.info["push_step"] + 1, state.info["push_interval_steps"]) == 0
        )
        # å¦‚æžœå…¨å±€å¼€å…³å…³é—­ï¼Œåˆ™æ¸…é›¶æŽ¨åŠ›
        push *= self._config.push_config.enable

        # å°†æŽ¨åŠ›æ·»åŠ åˆ°æµ®åŠ¨åŸºåº•çš„ qvel[x,y] ä¸Š
        qvel = state.data.qvel
        qvel = qvel.at[
               self._floating_base_qvel_addr : self._floating_base_qvel_addr + 2
               ].set(
            push * push_magnitude
            + qvel[self._floating_base_qvel_addr : self._floating_base_qvel_addr + 2]
        )
        data = state.data.replace(qvel=qvel)
        state = state.replace(data=data)

        # æ ¹æ®åŠ¨ä½œç”Ÿæˆ motor targetï¼Œaction æ˜¯å½’ä¸€åŒ–è¾“å‡ºï¼Œå› æ­¤ä¹˜ä»¥ action_scale å¹¶åç§»è‡³é»˜è®¤ä½ç½®
        # motor_targets = a_default + a_normalized * Î±ï¼Œå…¶ä¸­ Î± æ˜¯ self._config.action_scale
        motor_targets = (
                self._default_actuator + action_w_delay * self._config.action_scale
        )

        # å¦‚æžœå¯ç”¨äº†ç”µæœºæœ€å¤§é€Ÿåº¦é™åˆ¶ï¼Œåˆ™å¯¹ motor_targets åšé™é€Ÿè£å‰ª
        # motor_targets_clipped = clip(motor_targets, prev Â± v_max * dt)
        if USE_MOTOR_SPEED_LIMITS:
            prev_motor_targets = state.info["motor_targets"]
            motor_targets = jp.clip(
                motor_targets,
                prev_motor_targets - self._config.max_motor_velocity * self.dt,  # ä¸‹é™
                prev_motor_targets + self._config.max_motor_velocity * self.dt,  # ä¸Šé™
            )

        # ç”¨ motor_targets æ‰§è¡Œä»¿çœŸæ­¥è¿›ï¼Œæ‰§è¡Œ self.n_substeps ä¸ªå­æ­¥
        data = mjx_env.step(self.mjx_model, state.data, motor_targets, self.n_substeps)
        state.info["motor_targets"] = motor_targets  # å­˜å‚¨è¯¥æ­¥ç›®æ ‡å€¼ï¼ˆç”¨äºŽé™é€Ÿï¼‰

        # æ£€æµ‹åŒè„šæ˜¯å¦ä¸Žåœ°é¢æŽ¥è§¦
        contact = jp.array(
            [
                geoms_colliding(data, geom_id, self._floor_geom_id)
                for geom_id in self._feet_geom_id
            ]
        )
        # è®°å½•é¦–æ¬¡è½åœ°å¸§ï¼ˆä¸Šä¸€å¸§éžæŽ¥è§¦ï¼Œæœ¬å¸§æŽ¥è§¦ï¼‰
        contact_filt = contact | state.info["last_contact"]
        first_contact = (state.info["feet_air_time"] > 0.0) * contact_filt

        # è®°å½•ä¸¤è„šç©ºä¸­æŒç»­æ—¶é—´ä¸Žæœ€é«˜ç¦»åœ°é«˜åº¦ï¼ˆswing_peakï¼‰
        state.info["feet_air_time"] += self.dt
        p_f = data.site_xpos[self._feet_site_id]  # æ‰€æœ‰è„šçš„ä¸–ç•Œåæ ‡
        p_fz = p_f[..., -1]  # æå– z é«˜åº¦
        state.info["swing_peak"] = jp.maximum(state.info["swing_peak"], p_fz)

        # èŽ·å–å½“å‰è§‚æµ‹
        obs = self._get_obs(data, state.info, contact)
        # åˆ¤æ–­æ˜¯å¦æ‘”å€’
        done = self._get_termination(data)

        # è®¡ç®—æ‰€æœ‰å­å¥–åŠ±é¡¹ï¼ˆæœªåŠ æƒï¼‰
        rewards = self._get_reward(
            data, action, state.info, state.metrics, done, first_contact, contact
        )
        # æ¯é¡¹ä¹˜ä»¥é…ç½®ä¸­çš„æƒé‡ï¼Œreward_config.scales[k] å¯¹åº”æ¯ä¸€é¡¹ k çš„ç³»æ•°
        rewards = {
            k: v * self._config.reward_config.scales[k] for k, v in rewards.items()
        }
        # æ€»å¥–åŠ±ä¸º sum(rewards) * dtï¼Œå¹¶è£å‰ªæœ€å¤§å€¼ï¼Œé¿å…ä¸ç¨³å®š
        # reward_total = clip(Î£_k r_k * dt, 0, 10000)
        reward = jp.clip(sum(rewards.values()) * self.dt, 0.0, 10000.0)

        # æ›´æ–°çŠ¶æ€è®°å½•
        state.info["push"] = push
        state.info["step"] += 1
        state.info["push_step"] += 1
        state.info["last_last_last_act"] = state.info["last_last_act"]
        state.info["last_last_act"] = state.info["last_act"]
        state.info["last_act"] = action

        # æ¯ 500 æ­¥é‡æ–°é‡‡æ ·å‘½ä»¤ï¼ˆæ¨¡æ‹Ÿ joystick å˜åŒ–ï¼‰
        state.info["rng"], cmd_rng = jax.random.split(state.info["rng"])
        state.info["command"] = jp.where(
            state.info["step"] > 500,
            self.sample_command(cmd_rng),
            state.info["command"],
            )

        # è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶æˆ–æ­¥æ•°ä¸Šé™åŽæ¸…é›¶ step
        state.info["step"] = jp.where(
            done | (state.info["step"] > 500),
            0,
            state.info["step"],
            )

        # è‹¥æŽ¥è§¦åˆ™ feet_air_time å½’é›¶ï¼›æœªæŽ¥è§¦åˆ™ä¿æŒç´¯åŠ 
        state.info["feet_air_time"] *= ~contact
        state.info["last_contact"] = contact
        state.info["swing_peak"] *= ~contact

        # å°†å¥–åŠ±/æƒ©ç½šé¡¹è®°å½•è¿› metricsï¼Œåˆ† reward å’Œ cost
        for k, v in rewards.items():
            rew_scale = self._config.reward_config.scales[k]
            if rew_scale != 0:
                if rew_scale > 0:
                    state.metrics[f"reward/{k}"] = v
                else:
                    state.metrics[f"cost/{k}"] = -v

        # è®°å½•è¯¥å¸§æœ€å¤§æŠ¬è…¿é«˜åº¦
        state.metrics["swing_peak"] = jp.mean(state.info["swing_peak"])

        done = done.astype(reward.dtype)
        state = state.replace(data=data, obs=obs, reward=reward, done=done)
        return state

    def _get_termination(self, data: mjx.Data) -> jax.Array:
        fall_termination = self.get_gravity(data)[-1] < 0.0
        return fall_termination | jp.isnan(data.qpos).any() | jp.isnan(data.qvel).any()

    def _get_obs(
        self, data: mjx.Data, info: dict[str, Any], contact: jax.Array
    ) -> mjx_env.Observation:

        gyro = self.get_gyro(data)
        info["rng"], noise_rng = jax.random.split(info["rng"])
        noisy_gyro = (
            gyro
            + (2 * jax.random.uniform(noise_rng, shape=gyro.shape) - 1)
            * self._config.noise_config.level
            * self._config.noise_config.scales.gyro
        )

        accelerometer = self.get_accelerometer(data)
        # accelerometer[0] += 1.3 # TODO testing
        accelerometer.at[0].set(accelerometer[0] + 1.3)

        info["rng"], noise_rng = jax.random.split(info["rng"])
        noisy_accelerometer = (
            accelerometer
            + (2 * jax.random.uniform(noise_rng, shape=accelerometer.shape) - 1)
            * self._config.noise_config.level
            * self._config.noise_config.scales.accelerometer
        )

        gravity = data.site_xmat[self._site_id].T @ jp.array([0, 0, -1])
        info["rng"], noise_rng = jax.random.split(info["rng"])
        noisy_gravity = (
            gravity
            + (2 * jax.random.uniform(noise_rng, shape=gravity.shape) - 1)
            * self._config.noise_config.level
            * self._config.noise_config.scales.gravity
        )

        # Handle IMU delay
        imu_history = jp.roll(info["imu_history"], 3).at[:3].set(noisy_gravity)
        info["imu_history"] = imu_history
        imu_idx = jax.random.randint(
            noise_rng,
            (1,),
            minval=self._config.noise_config.imu_min_delay,
            maxval=self._config.noise_config.imu_max_delay,
        )
        noisy_gravity = imu_history.reshape((-1, 3))[imu_idx[0]]

        # joint_angles = data.qpos[7:]

        # Handling backlash
        joint_angles = self.get_actuator_joints_qpos(data.qpos)
        joint_backlash = self.get_actuator_backlash_qpos(data.qpos)

        for i in self.backlash_idx_to_add:
            joint_backlash = jp.insert(joint_backlash, i, 0)

        joint_angles = joint_angles + joint_backlash

        info["rng"], noise_rng = jax.random.split(info["rng"])
        noisy_joint_angles = (
            joint_angles
            + (2.0 * jax.random.uniform(noise_rng, shape=joint_angles.shape) - 1.0)
            * self._config.noise_config.level
            * self._qpos_noise_scale
        )

        # joint_vel = data.qvel[6:]
        joint_vel = self.get_actuator_joints_qvel(data.qvel)
        info["rng"], noise_rng = jax.random.split(info["rng"])
        noisy_joint_vel = (
            joint_vel
            + (2.0 * jax.random.uniform(noise_rng, shape=joint_vel.shape) - 1.0)
            * self._config.noise_config.level
            * self._config.noise_config.scales.joint_vel
        )

        linvel = self.get_local_linvel(data)
        # info["rng"], noise_rng = jax.random.split(info["rng"])
        # noisy_linvel = (
        #     linvel
        #     + (2 * jax.random.uniform(noise_rng, shape=linvel.shape) - 1)
        #     * self._config.noise_config.level
        #     * self._config.noise_config.scales.linvel
        # )

        state = jp.hstack(
            [
                # noisy_linvel,  # 3
                # noisy_gyro,  # 3
                # noisy_gravity,  # 3
                noisy_gyro,  # 3
                noisy_accelerometer,  # 3
                info["command"],  # 3
                noisy_joint_angles - self._default_actuator,  # 10
                noisy_joint_vel * self._config.dof_vel_scale,  # 10
                info["last_act"],  # 10
                info["last_last_act"],  # 10
                info["last_last_last_act"],  # 10
                info["motor_targets"],  # 10
                contact,  # 2
                # info["current_reference_motion"],
                # info["imitation_i"],
                info["imitation_phase"],
            ]
        )

        accelerometer = self.get_accelerometer(data)
        global_angvel = self.get_global_angvel(data)
        feet_vel = data.sensordata[self._foot_linvel_sensor_adr].ravel()
        root_height = data.qpos[self._floating_base_qpos_addr + 2]

        privileged_state = jp.hstack(
            [
                state,
                gyro,  # 3
                accelerometer,  # 3
                gravity,  # 3
                linvel,  # 3
                global_angvel,  # 3
                joint_angles - self._default_actuator,
                joint_vel,
                root_height,  # 1
                data.actuator_force,  # 10
                contact,  # 2
                feet_vel,  # 4*3
                info["feet_air_time"],  # 2
                info["current_reference_motion"],
                info["imitation_i"],
                info["imitation_phase"],
            ]
        )

        return {
            "state": state,
            "privileged_state": privileged_state,
        }

    # å†…éƒ¨è°ƒç”¨çš„å¥–åŠ±å‡½æ•°ï¼Œè¾“å‡ºæ‰€æœ‰ reward åç§°å¯¹åº”çš„å€¼ï¼ˆæœªåŠ æƒï¼‰
    def _get_reward(
        self,
        data: mjx.Data,
        action: jax.Array,
        info: dict[str, Any],
        metrics: dict[str, Any],
        done: jax.Array,
        first_contact: jax.Array,
        contact: jax.Array,
    ) -> dict[str, jax.Array]:
        #å½“å‰æœªä½¿ç”¨ metrics å‚æ•°ï¼Œé¿å… PyLint æŠ¥é”™ã€‚
        del metrics  # Unused.

        ret = {
            # å¥–åŠ±æœºå™¨äººä»¥æ­£ç¡®çš„ x/y å¹³ç§»é€Ÿåº¦ç§»åŠ¨ã€‚
            # å…¬å¼ï¼šr = exp( - ||v_target - v_actual||^2 / (2 * Ïƒ^2) )
            # æ¥è‡ªè®ºæ–‡ DeepMimic: https://xbpeng.github.io/projects/DeepMimic/
            "tracking_lin_vel": reward_tracking_lin_vel(
                info["command"], # joystick å‘½ä»¤ä¸­çš„çº¿é€Ÿåº¦ç›®æ ‡ vx, vy
                self.get_local_linvel(data), # å½“å‰èº«ä½“çš„æœ¬åœ°çº¿é€Ÿåº¦
                self._config.reward_config.tracking_sigma,  # reward å¹³æ»‘ç³»æ•°, é«˜æ–¯æƒ©ç½šé¡¹çš„Ïƒ
            ),

            # å¥–åŠ±æœºå™¨äººä»¥æ­£ç¡®çš„è§’é€Ÿåº¦ï¼ˆè½¬å‘ï¼‰ç§»åŠ¨ã€‚
            # å…¬å¼ï¼šr = exp( - ||Ï‰_target - Ï‰_actual||^2 / (2 * Ïƒ^2) )
            # ç”¨äºŽé¼“åŠ± agent åŽŸåœ°æˆ–è¡Œè¿›ä¸­è½¬å‘æ­£ç¡®ã€‚
            "tracking_ang_vel": reward_tracking_ang_vel(
                info["command"],# joystick å‘½ä»¤ä¸­çš„æœŸæœ›è§’é€Ÿåº¦ yaw_rate
                self.get_gyro(data),# å½“å‰é™€èžºä»ªè§’é€Ÿåº¦ï¼ˆé€šå¸¸ç»• zï¼‰
                self._config.reward_config.tracking_sigma,
            ),

            # "orientation": cost_orientation(self.get_gravity(data)),
            # æƒ©ç½šä½¿ç”¨è¿‡å¤§çš„å…³èŠ‚åŠ›çŸ©ï¼Œé¼“åŠ±èŠ‚èƒ½æŽ§åˆ¶ã€‚
            # å…¬å¼ï¼šcost = Î» * Î£_i Ï„_i^2
            # å‚è€ƒï¼šTassa et al., "Synthesis and stabilization of complex behaviors" (2012)
            "torques": cost_torques(data.actuator_force),

            # æƒ©ç½šè¿žç»­ä¸¤æ­¥åŠ¨ä½œå·®å¼‚è¿‡å¤§ï¼Œé¼“åŠ±æŽ§åˆ¶å¹³æ»‘ã€‚
            # å…¬å¼ï¼šcost = Î» * ||a_t - a_{t-1}||^2
            # å¯é˜²æ­¢æŒ¯è¡ã€é™ä½Žç¡¬ä»¶ç£¨æŸã€å¢žå¼º sim2real é²æ£’æ€§ã€‚
            "action_rate": cost_action_rate(action, info["last_act"]),

            # ç»™äºˆåŸºç¡€â€œå­˜æ´»å¥–åŠ±â€ï¼Œé¼“åŠ± agent ä¸æ‘”å€’ã€æŒç»­è¿åŠ¨ã€‚
            # é€šå¸¸è¿”å›žå›ºå®šå¸¸æ•°ï¼Œå¦‚ 1 æˆ– 20ã€‚
            # è‹¥å‘ç”Ÿæ‘”å€’ã€nanã€ç¿»æ»šç­‰ï¼Œä¼šåœ¨çŽ¯å¢ƒ step() ä¸­è¢«ç½®ä¸º doneã€‚
            "alive": reward_alive(),

            # imitation å¥–åŠ±ï¼šé¼“åŠ± agent æ¨¡ä»¿å‚è€ƒåŠ¨ä½œã€‚
            # é€šå¸¸åŒ…æ‹¬ï¼š
            # - å…³èŠ‚è§’åº¦è¯¯å·®ï¼šÎ£ ||q_ref - q||^2
            # - å…³èŠ‚é€Ÿåº¦è¯¯å·®ï¼šÎ£ ||dq_ref - dq||^2
            # - æ ¹éƒ¨ä½ç½®/é€Ÿåº¦å·®å¼‚
            # å¯æ ¹æ® AMP æ¡†æž¶æ›¿ä»£ discriminatorï¼Œç›´æŽ¥ä½¿ç”¨ imitation rewardã€‚
            # å‚è€ƒè®ºæ–‡ï¼š
            # - AMP: https://arxiv.org/abs/2104.02180
            # - DeepMimic: https://xbpeng.github.io/projects/DeepMimic/
            "imitation": reward_imitation(  # FIXME, this reward is so adhoc...
                self.get_floating_base_qpos(data.qpos),  # æ ¹éƒ¨ä½ç½® + å§¿æ€ï¼ˆå››å…ƒæ•°ï¼‰
                self.get_floating_base_qvel(data.qvel),  # æ ¹éƒ¨é€Ÿåº¦
                self.get_actuator_joints_qpos(data.qpos),# æ‰€æœ‰å…³èŠ‚è§’åº¦
                self.get_actuator_joints_qvel(data.qvel),# æ‰€æœ‰å…³èŠ‚é€Ÿåº¦
                contact, # å½“å‰æŽ¥è§¦çŠ¶æ€ï¼ˆç”¨äºŽå¹³è¡¡è„šæ­¥ï¼‰
                info["current_reference_motion"],# å¤šé¡¹å¼æ‹Ÿåˆç”Ÿæˆçš„å‚è€ƒåŠ¨ä½œè½¨è¿¹
                info["command"],# å½“å‰ joystick æŽ§åˆ¶å‘½ä»¤
                USE_IMITATION_REWARD,# imitation å¥–åŠ±æ˜¯å¦å¯ç”¨
            ),

            # å½“ç›®æ ‡å‘½ä»¤ä¸ºé›¶æ—¶ï¼Œæƒ©ç½šæœºå™¨äººç§»åŠ¨ã€‚
            # ç”¨äºŽé¼“åŠ± agent åœ¨â€œé™æ­¢å‘½ä»¤â€ä¸‹ä¿æŒç«™ç«‹ä¸åŠ¨ã€‚
            # å®žçŽ°é€»è¾‘ï¼šè‹¥ ||command|| â‰ˆ 0ï¼Œåˆ™å¯¹ q, dq ä¸Žé»˜è®¤å€¼è¿›è¡Œ L2 æƒ©ç½š
            "stand_still": cost_stand_still(
                # info["command"], data.qpos[7:], data.qvel[6:], self._default_pose
                info["command"],# joystick å‘½ä»¤
                self.get_actuator_joints_qpos(data.qpos), # å½“å‰å…³èŠ‚è§’åº¦
                self.get_actuator_joints_qvel(data.qvel),# å½“å‰å…³èŠ‚é€Ÿåº¦
                self._default_actuator, # é»˜è®¤é™æ­¢è§’åº¦ï¼ˆå…³é”®å¸§â€œhomeâ€å§¿æ€ï¼‰
                ignore_head=False, # æ˜¯å¦å¿½ç•¥å¤´éƒ¨ï¼ˆè¿™é‡Œä¸å¿½ç•¥ï¼‰
            ),
        }

        return ret

    def sample_command(self, rng: jax.Array) -> jax.Array:
        rng1, rng2, rng3, rng4, rng5, rng6, rng7, rng8 = jax.random.split(rng, 8)

        lin_vel_x = jax.random.uniform(
            rng1, minval=self._config.lin_vel_x[0], maxval=self._config.lin_vel_x[1]
        )
        lin_vel_y = jax.random.uniform(
            rng2, minval=self._config.lin_vel_y[0], maxval=self._config.lin_vel_y[1]
        )
        ang_vel_yaw = jax.random.uniform(
            rng3,
            minval=self._config.ang_vel_yaw[0],
            maxval=self._config.ang_vel_yaw[1],
        )

        neck_pitch = jax.random.uniform(
            rng5,
            minval=self._config.neck_pitch_range[0] * self._config.head_range_factor,
            maxval=self._config.neck_pitch_range[1] * self._config.head_range_factor,
        )

        head_pitch = jax.random.uniform(
            rng6,
            minval=self._config.head_pitch_range[0] * self._config.head_range_factor,
            maxval=self._config.head_pitch_range[1] * self._config.head_range_factor,
        )

        head_yaw = jax.random.uniform(
            rng7,
            minval=self._config.head_yaw_range[0] * self._config.head_range_factor,
            maxval=self._config.head_yaw_range[1] * self._config.head_range_factor,
        )

        head_roll = jax.random.uniform(
            rng8,
            minval=self._config.head_roll_range[0] * self._config.head_range_factor,
            maxval=self._config.head_roll_range[1] * self._config.head_range_factor,
        )

        # With 10% chance, set everything to zero.
        return jp.where(
            jax.random.bernoulli(rng4, p=0.1),
            jp.zeros(7),
            jp.hstack(
                [
                    lin_vel_x,
                    lin_vel_y,
                    ang_vel_yaw,
                    neck_pitch,
                    head_pitch,
                    head_yaw,
                    head_roll,
                ]
            ),
        )
